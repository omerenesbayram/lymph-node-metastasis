{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom \n",
        "!pip install pylibjpeg[all]"
      ],
      "metadata": {
        "id": "9oPQfQAt90sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvkYvzFERdnu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uK_aDehWRVC3"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import math\n",
        "import six\n",
        "from __future__ import (\n",
        "    absolute_import,\n",
        "    division,\n",
        "    print_function,\n",
        "    unicode_literals\n",
        ")\n",
        "from datetime import datetime\n",
        "from math import ceil\n",
        "\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    BatchNormalization\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv3D,\n",
        "    AveragePooling3D,\n",
        "    MaxPooling3D\n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from pydicom.pixel_data_handlers.util import apply_modality_lut\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_data_path = \"/content/drive/My Drive/Medical Research/Data_Tümü/\"\n",
        "drive_patient_list_path = \"/content/drive/My Drive/Medical Research/Hasta_Listesi.xlsx\"\n",
        "drive_log_path = \"/content/drive/My Drive/Medical Research/logs\""
      ],
      "metadata": {
        "id": "V9-jFZfO70yF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block (by @raghakot).\"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu3D(**conv_params):\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\n",
        "        \"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\",\n",
        "                                                l2(1e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv3D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, kernel_initializer=kernel_initializer,\n",
        "                      padding=padding,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv3d(**conv_params):\n",
        "    \"\"\"Helper to build a  BN -> relu -> conv3d block.\"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\",\n",
        "                                                \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\",\n",
        "                                                l2(1e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv3D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, kernel_initializer=kernel_initializer,\n",
        "                      padding=padding,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut3d(input, residual):\n",
        "    \"\"\"3D shortcut to match input and residual and merges them with \"sum\".\"\"\"\n",
        "    stride_dim1 = ceil(input.shape[DIM1_AXIS] \\\n",
        "        / residual.shape[DIM1_AXIS])\n",
        "    stride_dim2 = ceil(input.shape[DIM2_AXIS] \\\n",
        "        / residual.shape[DIM2_AXIS])\n",
        "    stride_dim3 = ceil(input.shape[DIM3_AXIS] \\\n",
        "        / residual.shape[DIM3_AXIS])\n",
        "    equal_channels = residual.shape[CHANNEL_AXIS] \\\n",
        "        == input.shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    if stride_dim1 > 1 or stride_dim2 > 1 or stride_dim3 > 1 \\\n",
        "            or not equal_channels:\n",
        "        shortcut = Conv3D(\n",
        "            filters=residual.shape[CHANNEL_AXIS],\n",
        "            kernel_size=(1, 1, 1),\n",
        "            strides=(stride_dim1, stride_dim2, stride_dim3),\n",
        "            kernel_initializer=\"he_normal\", padding=\"valid\",\n",
        "            kernel_regularizer=l2(1e-4)\n",
        "            )(input)\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block3d(block_function, filters, kernel_regularizer, repetitions,\n",
        "                      is_first_layer=False):\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            strides = (1, 1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                strides = (2, 2, 2)\n",
        "            input = block_function(filters=filters, strides=strides,\n",
        "                                   kernel_regularizer=kernel_regularizer,\n",
        "                                   is_first_block_of_first_layer=(\n",
        "                                       is_first_layer and i == 0)\n",
        "                                   )(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, strides=(1, 1, 1), kernel_regularizer=l2(1e-4),\n",
        "                is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 X 3 convolution blocks. Extended from raghakot's 2D impl.\"\"\"\n",
        "    def f(input):\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv3D(filters=filters, kernel_size=(3, 3, 3),\n",
        "                           strides=strides, padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=kernel_regularizer\n",
        "                           )(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv3d(filters=filters,\n",
        "                                    kernel_size=(3, 3, 3),\n",
        "                                    strides=strides,\n",
        "                                    kernel_regularizer=kernel_regularizer\n",
        "                                    )(input)\n",
        "\n",
        "        residual = _bn_relu_conv3d(filters=filters, kernel_size=(3, 3, 3),\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   )(conv1)\n",
        "        return _shortcut3d(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, strides=(1, 1, 1), kernel_regularizer=l2(1e-4),\n",
        "               is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 X 3 convolution blocks. Extended from raghakot's 2D impl.\"\"\"\n",
        "    def f(input):\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv3D(filters=filters, kernel_size=(1, 1, 1),\n",
        "                              strides=strides, padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=kernel_regularizer\n",
        "                              )(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv3d(filters=filters, kernel_size=(1, 1, 1),\n",
        "                                       strides=strides,\n",
        "                                       kernel_regularizer=kernel_regularizer\n",
        "                                       )(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv3d(filters=filters, kernel_size=(3, 3, 3),\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   )(conv_1_1)\n",
        "        residual = _bn_relu_conv3d(filters=filters * 4, kernel_size=(1, 1, 1),\n",
        "                                   kernel_regularizer=kernel_regularizer\n",
        "                                   )(conv_3_3)\n",
        "\n",
        "        return _shortcut3d(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _handle_data_format():\n",
        "    global DIM1_AXIS\n",
        "    global DIM2_AXIS\n",
        "    global DIM3_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        DIM1_AXIS = 1\n",
        "        DIM2_AXIS = 2\n",
        "        DIM3_AXIS = 3\n",
        "        CHANNEL_AXIS = 4\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        DIM1_AXIS = 2\n",
        "        DIM2_AXIS = 3\n",
        "        DIM3_AXIS = 4\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class Resnet3DBuilder(object):\n",
        "    \"\"\"ResNet3D.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions, reg_factor):\n",
        "        \"\"\"Instantiate a vanilla ResNet3D keras model.\n",
        "        # Arguments\n",
        "            input_shape: Tuple of input shape in the format\n",
        "            (conv_dim1, conv_dim2, conv_dim3, channels) if dim_ordering='tf'\n",
        "            (filter, conv_dim1, conv_dim2, conv_dim3) if dim_ordering='th'\n",
        "            num_outputs: The number of outputs at the final softmax layer\n",
        "            block_fn: Unit block to use {'basic_block', 'bottlenack_block'}\n",
        "            repetitions: Repetitions of unit blocks\n",
        "        # Returns\n",
        "            model: a 3D ResNet model that takes a 5D tensor (volumetric images\n",
        "            in batch) as input and returns a 1D vector (prediction) as output.\n",
        "        \"\"\"\n",
        "        _handle_data_format()\n",
        "        if len(input_shape) != 4:\n",
        "            raise ValueError(\"Input shape should be a tuple \"\n",
        "                             \"(conv_dim1, conv_dim2, conv_dim3, channels) \"\n",
        "                             \"for tensorflow as backend or \"\n",
        "                             \"(channels, conv_dim1, conv_dim2, conv_dim3) \"\n",
        "                             \"for theano as backend\")\n",
        "\n",
        "        block_fn = _get_block(block_fn)\n",
        "        input = Input(shape=input_shape)\n",
        "        # first conv\n",
        "        conv1 = _conv_bn_relu3D(filters=64, kernel_size=(7, 7, 7),\n",
        "                                strides=(2, 2, 2),\n",
        "                                kernel_regularizer=l2(reg_factor)\n",
        "                                )(input)\n",
        "        pool1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2),\n",
        "                             padding=\"same\")(conv1)\n",
        "\n",
        "        # repeat blocks\n",
        "        block = pool1\n",
        "        filters = 64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            block = _residual_block3d(block_fn, filters=filters,\n",
        "                                      kernel_regularizer=l2(reg_factor),\n",
        "                                      repetitions=r, is_first_layer=(i == 0)\n",
        "                                      )(block)\n",
        "            filters *= 2\n",
        "\n",
        "        # last activation\n",
        "        block_output = _bn_relu(block)\n",
        "\n",
        "        # average poll and classification\n",
        "        pool2 = AveragePooling3D(pool_size=(block.shape[DIM1_AXIS],\n",
        "                                            block.shape[DIM2_AXIS],\n",
        "                                            block.shape[DIM3_AXIS]),\n",
        "                                 strides=(1, 1, 1))(block_output)\n",
        "        flatten1 = Flatten()(pool2)\n",
        "        if num_outputs > 1:\n",
        "            dense = Dense(units=num_outputs,\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          activation=\"softmax\",\n",
        "                          kernel_regularizer=l2(reg_factor))(flatten1)\n",
        "        else:\n",
        "            dense = Dense(units=num_outputs,\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          activation=\"sigmoid\",\n",
        "                          kernel_regularizer=l2(reg_factor))(flatten1)\n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs, reg_factor=1e-4):\n",
        "        \"\"\"Build resnet 18.\"\"\"\n",
        "        return Resnet3DBuilder.build(input_shape, num_outputs, basic_block,\n",
        "                                     [2, 2, 2, 2], reg_factor=reg_factor)\n"
      ],
      "metadata": {
        "id": "CaQLWCRk06tE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(name, type, path):\n",
        "    df = pd.read_excel(path)\n",
        "    df[\"Name\"] = df[\"Name\"].str.lower()\n",
        "    df = df.loc[df['Name'] == name]\n",
        "    \n",
        "    l1 = df[[\"2R\"]].iloc[0].to_numpy()\n",
        "    l2 = df[[\"4R\"]].iloc[0].to_numpy()\n",
        "    l3 = df[[7]].iloc[0].to_numpy()\n",
        "    l4 = df[[\"2L\"]].iloc[0].to_numpy()\n",
        "    l5 = df[[\"4L\"]].iloc[0].to_numpy()\n",
        "\n",
        "    df = l1 or l2 or l3 or l4 or l5\n",
        "\n",
        "    if(type == \"train\"):\n",
        "        if df == 0:\n",
        "            return np.array([1,0])\n",
        "        else:\n",
        "            return np.array([0,1])\n",
        "    if(type == \"predict\"):\n",
        "        return df\n",
        "\n",
        "\n",
        "def get_names(path):\n",
        "    \n",
        "    df = pd.read_excel(path)\n",
        "    df[\"Name\"] = df[\"Name\"].str.lower()\n",
        "\n",
        "    df = df[[\"Name\", \"Uygunluk\"]]\n",
        "    df = df[df.Uygunluk == 1]\n",
        "    df = df.dropna()\n",
        "\n",
        "    df = df[\"Name\"]\n",
        "    df = df.to_numpy()\n",
        "\n",
        "    pos = []\n",
        "    neg = []\n",
        "\n",
        "    for name in df:\n",
        "        label = get_label(name, \"predict\", path)\n",
        "        if(label == 1):\n",
        "            pos.append(name)\n",
        "        else:\n",
        "            neg.append(name)\n",
        "\n",
        "    return pos, neg"
      ],
      "metadata": {
        "id": "VMdJp-7C1_-X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_MIN = -140\n",
        "DATA_MAX = 260\n",
        "\n",
        "def cut(data, mid, size):\n",
        "    mid_x, mid_y, mid_z = mid\n",
        "    min_x = max(0, math.floor(mid_x - size // 2))\n",
        "    max_x = min(math.ceil(mid_x + size // 2), data.shape[0]-1)\n",
        "    min_y = max(0, math.floor(mid_y - size // 2))\n",
        "    max_y = min(math.ceil(mid_y + size // 2), data.shape[1]-1)\n",
        "    min_z = max(0, math.floor(mid_z - size // 2))\n",
        "    max_z = min(math.ceil(mid_z + size // 2), data.shape[2]-1)\n",
        "    data = data[min_x:max_x, min_y:max_y, min_z:max_z]\n",
        "    return data\n",
        "\n",
        "def normalize(data):\n",
        "    data[data > DATA_MAX] = DATA_MAX\n",
        "    data[data < DATA_MIN] = DATA_MIN\n",
        "    data = (data - DATA_MIN) / (DATA_MAX - DATA_MIN)\n",
        "    return data\n",
        "\n",
        "def find_mid(data):\n",
        "    non_zero = np.nonzero(data)\n",
        "    x = non_zero[0]\n",
        "    y = non_zero[1]\n",
        "    z = non_zero[2]\n",
        "    mid_z = (max(z) + min(z)) // 2\n",
        "    mid_y = (max(y) + min(y)) // 2\n",
        "    mid_x = (max(x) + min(x)) // 2\n",
        "    return (mid_x, mid_y, mid_z)\n",
        "\n",
        "def padding(data, size):\n",
        "    x, y, z = data.shape\n",
        "    img = np.zeros((size, size, size))\n",
        "    x_diff = (size-x) // 2\n",
        "    y_diff = (size-y) // 2\n",
        "    z_diff = (size-z) // 2\n",
        "    img[x_diff:x_diff+x, y_diff:y_diff+y, z_diff:z_diff+z] = data\n",
        "    return img\n",
        "\n",
        "def broadcast_to_shape(data, shape):\n",
        "    x = data.shape[0] == shape\n",
        "    y = data.shape[1] == shape\n",
        "    z = data.shape[2] == shape\n",
        "    if not x:\n",
        "        diff = (shape - data.shape[0])\n",
        "        data = np.pad(data, [(diff // 2, diff - (diff // 2)),(0,0),(0,0)], mode = \"constant\")\n",
        "    if not y:\n",
        "        diff = (shape - data.shape[1])\n",
        "        data = np.pad(data, [(0,0),(diff // 2, diff - (diff // 2)),(0,0)], mode = \"constant\")\n",
        "    if not x:\n",
        "        diff = (shape - data.shape[2])\n",
        "        data = np.pad(data, [(0,0),(0,0),(diff // 2, diff - (diff // 2))], mode = \"constant\")\n",
        "    return data\n",
        "\n",
        "def read_dicom(name, size, mid):\n",
        "    path = drive_data_path + name + \"/*.dcm\"\n",
        "    dcm_files = glob.glob(path)\n",
        "    pixel_data = []\n",
        "    data = []\n",
        "    for dcm_file in dcm_files:\n",
        "        dataset = pydicom.dcmread(dcm_file)\n",
        "        data.append(dataset)\n",
        "    slices = sorted(data, key=lambda s: s.ImagePositionPatient[2])\n",
        "    for slice in slices:\n",
        "        pixel_array = slice.pixel_array\n",
        "        pixel_data.append(apply_modality_lut(pixel_array, slice))\n",
        "    pixel_arr = np.asarray(pixel_data)\n",
        "    dcm_data = cut(pixel_arr, mid, size).astype(\"float32\")\n",
        "    dcm_data = normalize(dcm_data).astype(\"float32\")\n",
        "    x = np.empty((size, size, size, 1))\n",
        "    x[..., 0] = dcm_data\n",
        "    return x\n",
        "\n",
        "def read_data(name, size, segment, is_whole):\n",
        "    nifti_path = drive_data_path + name + \"/*.nii.gz\"\n",
        "    nifti_file = glob.glob(nifti_path)\n",
        "    img = nib.load(nifti_file[0])\n",
        "    data = img.get_fdata().transpose()\n",
        "    if not is_whole:\n",
        "        data[data != float(segment)] = 0.\n",
        "    mid = find_mid(data)\n",
        "    dcm = read_dicom(name, size, mid)\n",
        "    data = cut(data, mid, size)\n",
        "    return dcm, data"
      ],
      "metadata": {
        "id": "wS8zKsVf4wjj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, gen_type, name_list_path, segment=0.,is_whole=True, \n",
        "                 size = 100, batch_size=1, n_channels=1, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = (size,size,size)\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.size = size\n",
        "        self.segment = segment\n",
        "        self.is_whole = is_whole\n",
        "        self.gen_type = gen_type\n",
        "        self.name_list_path = name_list_path\n",
        "        self.time = datetime.now().strftime(\"%m-%d-%Y %H-%M-%S\")\n",
        "       \n",
        "   \n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(self.list_IDs.shape[0])\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return math.ceil(self.list_IDs.shape[0] / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        list_IDs_temp = self.list_IDs[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # Generate data\n",
        "      \n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X,y\n",
        "\n",
        "    def __iter__(self):\n",
        "        for item in (self[i] for i in range(len(self))):\n",
        "            yield item\n",
        " \n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.zeros((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.zeros((self.batch_size,2))\n",
        "        \n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            ID = ID.lower()\n",
        "            try:\n",
        "                y[i] = get_label(ID,self.gen_type,self.name_list_path)\n",
        "                X[i,], _ = read_data(ID,self.size,self.segment,self.is_whole)\n",
        "            except Exception as e:\n",
        "                file_name = drive_log_path + f\"/log_{self.time}.txt\"\n",
        "                with open(file_name, 'a+') as f:\n",
        "                    f.write(ID + \"\\n\\n\\n\")\n",
        "                    f.write(str(e))\n",
        "                    f.write(\"\\n---------------------------\\n\\n\\n\")\n",
        "\n",
        "        return X,y"
      ],
      "metadata": {
        "id": "yY_Lwyrl2hxN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JRv9sa91RVC6"
      },
      "outputs": [],
      "source": [
        "pos, neg = get_names(drive_patient_list_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j0WhY1dHRVC7"
      },
      "outputs": [],
      "source": [
        "neg = neg[:len(pos)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xkGJoEEjRVC8"
      },
      "outputs": [],
      "source": [
        "train_names_pos, val_names_pos = tts(pos, test_size=0.2, random_state=42)\n",
        "train_names_neg, val_names_neg = tts(neg, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BlX5l-8ERVC-"
      },
      "outputs": [],
      "source": [
        "train_names = np.asarray(train_names_pos + train_names_neg)\n",
        "np.random.shuffle(train_names)\n",
        "val_names = np.asarray(val_names_pos + val_names_neg)\n",
        "np.random.shuffle(val_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = DataGenerator(train_names, \"train\", drive_patient_list_path, batch_size=1)\n",
        "val_gen = DataGenerator(val_names, \"train\", drive_patient_list_path, batch_size=1)"
      ],
      "metadata": {
        "id": "9Pkf-lJOACyZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "daQMgqgLRVDJ"
      },
      "outputs": [],
      "source": [
        "model = Resnet3DBuilder.build_resnet_18((100, 100, 100, 1), 2)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.0001),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "IKsgTebhRVDJ"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import TensorBoard\n",
        "tensorboard = TensorBoard(log_dir=drive_log_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xdE0FpLRVDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b0748f-3fba-4c5c-aedd-edaf6ac6a0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        }
      ],
      "source": [
        "model.fit(train_gen, batch_size=1, epochs=50, validation_data=val_gen, callbacks = [tensorboard], verbose=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "resnet.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3b09108e514517bd5e4a6e228723cfb42d49fc72c35aeaac957cf3be0497138a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.11 64-bit ('py38': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}